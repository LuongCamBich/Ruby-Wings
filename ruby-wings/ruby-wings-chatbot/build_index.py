#!/usr/bin/env python3
"""
build_index.py - Táº¡o Ä‘áº§y Ä‘á»§ cÃ¡c file index cho Ruby Wings Chatbot
PhiÃªn báº£n Ä‘Æ¡n giáº£n, dá»… hiá»ƒu, tÆ°Æ¡ng thÃ­ch vá»›i app.py
"""

import os
import json
import numpy as np
import re
from typing import List, Dict, Any
from datetime import datetime
import unicodedata
import hashlib

# =========== CONFIGURATION ===========
# Äá»c tá»« biáº¿n mÃ´i trÆ°á»ng, náº¿u khÃ´ng cÃ³ dÃ¹ng giÃ¡ trá»‹ máº·c Ä‘á»‹nh
KNOWLEDGE_PATH = os.environ.get("KNOWLEDGE_PATH", "knowledge.json")
FAISS_INDEX_PATH = os.environ.get("FAISS_INDEX_PATH", "faiss_index.bin")
FAISS_MAPPING_PATH = os.environ.get("FAISS_MAPPING_PATH", "faiss_mapping.json")
FAISS_META_PATH = os.environ.get("FAISS_META_PATH", "faiss_index_meta.json")
FALLBACK_VECTORS_PATH = os.environ.get("FALLBACK_VECTORS_PATH", "vectors.npz")
OLD_FAISS_PATH = "index.faiss"  # Giá»¯ cá»‘ Ä‘á»‹nh cho backward compatibility

# OpenAI config - Ä‘á»c tá»« biáº¿n mÃ´i trÆ°á»ng
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY", "").strip()
USE_OPENAI = bool(OPENAI_API_KEY)

# Model config - Ä‘á»c tá»« biáº¿n mÃ´i trÆ°á»ng
EMBEDDING_MODEL = os.environ.get("EMBEDDING_MODEL", "text-embedding-3-small")
CHAT_MODEL = os.environ.get("CHAT_MODEL", "gpt-4o-mini")

# Kiá»ƒm tra cÃ¡c biáº¿n mÃ´i trÆ°á»ng quan trá»ng
print("ğŸ” KIá»‚M TRA BIáº¾N MÃ”I TRÆ¯á»œNG:")
print(f"   KNOWLEDGE_PATH: {KNOWLEDGE_PATH}")
print(f"   FAISS_INDEX_PATH: {FAISS_INDEX_PATH}")
print(f"   FAISS_MAPPING_PATH: {FAISS_MAPPING_PATH}")
print(f"   FALLBACK_VECTORS_PATH: {FALLBACK_VECTORS_PATH}")
print(f"   EMBEDDING_MODEL: {EMBEDDING_MODEL}")
print(f"   CHAT_MODEL: {CHAT_MODEL}")
print(f"   OPENAI_API_KEY cÃ³ tá»“n táº¡i: {bool(OPENAI_API_KEY)}")
print(f"   GOOGLE_SERVICE_ACCOUNT_JSON cÃ³ tá»“n táº¡i: {bool(os.environ.get('GOOGLE_SERVICE_ACCOUNT_JSON'))}")


# =========== FLATTEN KNOWLEDGE ===========
def load_and_flatten_knowledge(knowledge_path: str) -> tuple:
    """
    Äá»c knowledge.json vÃ  flatten thÃ nh danh sÃ¡ch vÄƒn báº£n
    Tráº£ vá»: (FLAT_TEXTS, MAPPING)
    """
    print(f"ğŸ“– Äang Ä‘á»c {knowledge_path}...")
    
    try:
        with open(knowledge_path, "r", encoding="utf-8") as f:
            KNOW = json.load(f)
        print(f"âœ… Äá»c thÃ nh cÃ´ng: {len(KNOW)} keys trong knowledge")
    except Exception as e:
        print(f"âŒ Lá»—i Ä‘á»c {knowledge_path}: {e}")
        return [], []
    
    FLAT_TEXTS = []
    MAPPING = []
    
    def scan(obj, prefix="root"):
        if isinstance(obj, dict):
            for k, v in obj.items():
                scan(v, f"{prefix}.{k}")
        elif isinstance(obj, list):
            for i, v in enumerate(obj):
                scan(v, f"{prefix}[{i}]")
        elif isinstance(obj, str):
            t = obj.strip()
            if t:
                FLAT_TEXTS.append(t)
                MAPPING.append({"path": prefix, "text": t})
        else:
            try:
                s = str(obj).strip()
                if s:
                    FLAT_TEXTS.append(s)
                    MAPPING.append({"path": prefix, "text": s})
            except Exception:
                pass
    
    scan(KNOW)
    print(f"âœ… Flatten thÃ nh cÃ´ng: {len(FLAT_TEXTS)} passages")
    return FLAT_TEXTS, MAPPING

# =========== EMBEDDINGS ===========
def deterministic_embedding(text: str, dim: int = 1536) -> List[float]:
    """
    Táº¡o embedding deterministic (á»•n Ä‘á»‹nh) cho text
    DÃ¹ng khi khÃ´ng cÃ³ OpenAI API
    """
    if not text:
        return [0.0] * dim
    
    # Láº¥y 2000 kÃ½ tá»± Ä‘áº§u
    short = text if len(text) <= 2000 else text[:2000]
    
    # Táº¡o hash á»•n Ä‘á»‹nh
    h = abs(hash(short)) % (10 ** 12)
    
    # Táº¡o vector dá»±a trÃªn hash
    vec = []
    for i in range(dim):
        # DÃ¹ng hash vÃ  index Ä‘á»ƒ táº¡o giÃ¡ trá»‹ pseudo-random
        val = ((h >> (i % 32)) & 0xFF) / 255.0
        # ThÃªm má»™t chÃºt variation dá»±a trÃªn kÃ½ tá»±
        if i < len(short):
            val = (val + ord(short[i % len(short)]) / 255.0) / 2.0
        vec.append(val)
    
    # Normalize
    norm = sum(v*v for v in vec) ** 0.5
    if norm > 0:
        vec = [v/norm for v in vec]
    
    return vec

def embed_with_openai(text: str, client, model: str = "text-embedding-3-small") -> List[float]:
    """
    Táº¡o embedding báº±ng OpenAI API
    """
    if not text:
        return []
    
    short = text if len(text) <= 2000 else text[:2000]
    
    try:
        resp = client.embeddings.create(
            model=model,
            input=short
        )
        if resp.data and len(resp.data) > 0:
            return resp.data[0].embedding
    except Exception as e:
        print(f"âš ï¸ OpenAI embedding lá»—i: {e}")
    
    return []

# =========== BUILD INDEXES ===========
def build_faiss_index(flat_texts: List[str], mapping: List[Dict]) -> bool:
    """
    XÃ¢y dá»±ng FAISS index vÃ  lÆ°u cÃ¡c file liÃªn quan
    """
    print("\nğŸ”§ Äang xÃ¢y dá»±ng FAISS index...")
    
    # Kiá»ƒm tra FAISS
    try:
        import faiss
        print("âœ… FAISS library cÃ³ sáºµn")
    except ImportError:
        print("âŒ FAISS khÃ´ng cÃ i Ä‘áº·t. Cháº¡y: pip install faiss-cpu")
        return False
    
    # Kiá»ƒm tra OpenAI
    client = None
    if USE_OPENAI:
        try:
            from openai import OpenAI
            client = OpenAI(api_key=OPENAI_API_KEY)
            print("âœ… OpenAI client khá»Ÿi táº¡o thÃ nh cÃ´ng")
        except Exception as e:
            print(f"âš ï¸ OpenAI client lá»—i: {e}")
            client = None
    else:
        print("â„¹ï¸  Sá»­ dá»¥ng deterministic embedding (khÃ´ng cÃ³ OpenAI API)")
    
    # Táº¡o embeddings
    print(f"ğŸ“Š Táº¡o embeddings cho {len(flat_texts)} passages...")
    
    vectors = []
    for i, text in enumerate(flat_texts):
        if i % 50 == 0:
            print(f"  Äang xá»­ lÃ½ {i}/{len(flat_texts)}...")
        
        if client:
            emb = embed_with_openai(text, client, EMBEDDING_MODEL)
        else:
            emb = deterministic_embedding(text, 1536)
        
        if emb:
            vectors.append(np.array(emb, dtype="float32"))
        else:
            # Fallback deterministic
            vectors.append(np.array(deterministic_embedding(text, 1536), dtype="float32"))
    
    if not vectors:
        print("âŒ KhÃ´ng táº¡o Ä‘Æ°á»£c vectors nÃ o")
        return False
    
    # Táº¡o matrix
    mat = np.vstack(vectors).astype("float32")
    print(f"âœ… Táº¡o matrix: {mat.shape[0]} vectors, {mat.shape[1]} dimensions")
    
    # Normalize cho cosine similarity
    row_norms = np.linalg.norm(mat, axis=1, keepdims=True)
    mat = mat / (row_norms + 1e-12)
    
    # Táº¡o FAISS index
    dim = mat.shape[1]
    index = faiss.IndexFlatIP(dim)  # Inner Product = cosine similarity
    index.add(mat)
    
    # =========== LÆ¯U CÃC FILE ===========
    
    # 1. LÆ°u FAISS index chÃ­nh
    print(f"\nğŸ’¾ Äang lÆ°u FAISS index vÃ o {FAISS_INDEX_PATH}...")
    faiss.write_index(index, FAISS_INDEX_PATH)
    print(f"âœ… ÄÃ£ lÆ°u: {FAISS_INDEX_PATH}")
    
    # 2. LÆ°u mapping
    print(f"ğŸ’¾ Äang lÆ°u mapping vÃ o {FAISS_MAPPING_PATH}...")
    with open(FAISS_MAPPING_PATH, "w", encoding="utf-8") as f:
        json.dump(mapping, f, ensure_ascii=False, indent=2)
    print(f"âœ… ÄÃ£ lÆ°u: {FAISS_MAPPING_PATH}")
    
    # 3. LÆ°u metadata
    print(f"ğŸ’¾ Äang lÆ°u metadata vÃ o {FAISS_META_PATH}...")
    meta_data = {
        "embedding_model": EMBEDDING_MODEL,
        "dimension": int(dim),
        "total_vectors": len(flat_texts),
        "created_at": datetime.now().isoformat(),
        "index_type": "IndexFlatIP",
        "normalized": True,
        "similarity_metric": "cosine",
        "has_openai": USE_OPENAI,
        "version": "2.1.1"
    }
    with open(FAISS_META_PATH, "w", encoding="utf-8") as f:
        json.dump(meta_data, f, indent=2)
    print(f"âœ… ÄÃ£ lÆ°u: {FAISS_META_PATH}")
    
    # 4. LÆ°u file FAISS cÅ© (backward compatibility)
    print(f"ğŸ’¾ Äang lÆ°u backup index vÃ o {OLD_FAISS_PATH}...")
    faiss.write_index(index, OLD_FAISS_PATH)
    print(f"âœ… ÄÃ£ lÆ°u: {OLD_FAISS_PATH}")
    
    # 5. LÆ°u numpy vectors (fallback)
    print(f"ğŸ’¾ Äang lÆ°u numpy vectors vÃ o {FALLBACK_VECTORS_PATH}...")
    np.savez_compressed(FALLBACK_VECTORS_PATH, mat=mat)
    print(f"âœ… ÄÃ£ lÆ°u: {FALLBACK_VECTORS_PATH}")
    
    # ThÃ´ng tin tá»•ng káº¿t
    print("\n" + "="*60)
    print("ğŸ‰ HOÃ€N THÃ€NH XÃ‚Y Dá»°NG INDEX!")
    print("="*60)
    print(f"ğŸ“Š Tá»•ng sá»‘ passages: {len(flat_texts)}")
    print(f"ğŸ“ Dimension: {dim}")
    print(f"ğŸ”¢ Index size: {index.ntotal} vectors")
    print(f"ğŸ”§ Embedding method: {'OpenAI' if USE_OPENAI else 'Deterministic'}")
    print(f"ğŸ’¾ CÃ¡c file Ä‘Ã£ táº¡o:")
    print(f"  1. {FAISS_INDEX_PATH} (FAISS index chÃ­nh)")
    print(f"  2. {FAISS_MAPPING_PATH} (mapping vÄƒn báº£n)")
    print(f"  3. {FAISS_META_PATH} (metadata)")
    print(f"  4. {OLD_FAISS_PATH} (FAISS index cÅ© - compatibility)")
    print(f"  5. {FALLBACK_VECTORS_PATH} (numpy fallback)")
    print("="*60)
    
    return True

# =========== MAIN ===========
def main():
    print("="*60)
    print("ğŸš€ RUBY WINGS - BUILD INDEX UTILITY")
    print("="*60)
    
    # Kiá»ƒm tra knowledge.json
    if not os.path.exists(KNOWLEDGE_PATH):
        print(f"âŒ KhÃ´ng tÃ¬m tháº¥y {KNOWLEDGE_PATH}")
        print(f"   Vui lÃ²ng Ä‘áº£m báº£o file knowledge.json tá»“n táº¡i trong thÆ° má»¥c nÃ y")
        return
    
    # 1. Äá»c vÃ  flatten knowledge
    flat_texts, mapping = load_and_flatten_knowledge(KNOWLEDGE_PATH)
    if not flat_texts:
        print("âŒ KhÃ´ng cÃ³ dá»¯ liá»‡u Ä‘á»ƒ xÃ¢y dá»±ng index")
        return
    
    # 2. XÃ¢y dá»±ng FAISS index
    success = build_faiss_index(flat_texts, mapping)
    
    if success:
        print("\nâœ¨ Táº¤T Cáº¢ HOÃ€N THÃ€NH! Báº¡n cÃ³ thá»ƒ cháº¡y app.py bÃ¬nh thÆ°á»ng.")
        print("ğŸ‘‰ Cháº¡y: python app.py")
    else:
        print("\nâŒ CÃ³ lá»—i xáº£y ra khi xÃ¢y dá»±ng index")

if __name__ == "__main__":
    main()